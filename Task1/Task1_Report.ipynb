{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minerva - Task 1 Report\n",
    "===\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "In Task One, we generated adversarial examples (AE) in the context of the zero-knowledge threat models. Using the Carlini - Wagner (CW), the Basic Iterative Method (BIM), and the Projected Gradient Descent (PGD) attacks, we crafted 18,000 adversarial examples that were derived from 18 attacks on 1000 adversarial examples. Our experiment has shown that a PGD attack is the most effective in deceiving an undefended model (UM). While PGD used miniscule epsilon values, a CW attack is most effective against an UM with a constant epsilon causing the error rate to increase gradually and a BIM attack is the most effective against an UM as the epsilon values increase. We have concluded that while BIM and CW attacks fool the UM with a 100% success rate, the PGD attack exhibited the highest rate of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "For this task, we decided to use increasing strength variants of the Carlini - Wagner attack (CW), the Basic Iterative Method (BIM) attack, and the Projected Gradient Descent (PGD) attack. In total, 18 attacks were used on a sample size of 1000 adversarial examples (AE), effectively creating 18,000 adversarial examples. These AE's were then tested for robustness on an undefended model, the Athena Framework, and the PGD mode. As the group had no previous experience with any of the attack methods and tunable parameters therein the attacks were chosen at random and the tunable parameters were chosen in a way that would provide a gradual increase in error rate on an undefended model.\n",
    "\n",
    "As an Aside:  \n",
    "No members of this group have post-graduate level knowledge of the inner workings of these attacks and as such it is worth mentioning the results found are only analyzed with the pure data produced from testing. In no way is the findings of this research to be compared with all attacks ever created or to be created. At no point should one assume our results have found \"the most effective attack\" by fixing some properties of adversarial implementations. The results we report are the results we found for the parameters we tested as we do not have the necessary research accumen to account for all possible adversarial attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "**The Carlini - Wagner attack** (CW) includes three possible distance metrics L<sub>0</sub>, L<sub>2</sub>, and L<sub>∞</sub>. L<sub>0</sub> corresponds to how many pixels have been altered in an image, L<sub>2</sub> measures the standard Euclidean distance between pixels that have been altered, and L<sub>∞</sub> measures the maximum change to any of the coordinates within the image following the following maximization:\n",
    "<p style=\"text-align: center;\"> || x - x' ||<sub>∞</sub> = max(|x<sub>1</sub>-x'<sub>1</sub>|,...,|x<sub>n</sub> - x'<sub>n</sub>|)</p>\n",
    "\n",
    "\n",
    "[\\[1\\]](https://arxiv.org/pdf/1608.04644.pdf)  \n",
    "In other words, the image has a maximum budget allowed but may have as many pixel changes as needed to meet that budget. The specific iteration that CW implements for their L<sub>∞</sub> attacks solve the minimization for\n",
    "<p style=\"text-align: center;\"> min c • ƒ(x + δ) + Σ<sub>i</sub>[(δ<sub>i</sub>-τ)<sup>+</sup>]</p><br>\n",
    "\n",
    "where τ is a correction to prevent oscillation in suboptimal solutions and c is the smallest value which results in which ƒ(x<sup>*</sup>) ≤ 0 [\\[1\\]](https://arxiv.org/pdf/1608.04644.pdf)  \n",
    "\n",
    "**The Basic Iterative Method** (BIM) is an extension of the Fast Gradiant (FGSM) method to produce adversarial attacks that introduced an iterative step in which the FGSM perturbations are applied multiple times in small steps and then pixel values of intermediate results are clipped after each step to ensure the resulting image is in an acceptible range of the original image [\\[2\\]](https://arxiv.org/pdf/1607.02533.pdf). This is mathematically represented as such\n",
    "\n",
    "<p style=\"text-align: center;\">X<sub>0</sub><sup>adv</sup> = X, X<sub>N+1</sub><sup>adv</sup> = Clip<sub>X,ε</sub> {X<sub>N</sub><sup>adv</sup> + α sign (∇<sub>X</sub>J(X<sub>N</sub><sup>adv</sup>, Y<sub>true</sub>)) } </p>\n",
    "\n",
    "Where, X is the image and X<sup>adv</sup> is the adversarial image, Y<sub>true</sub> is the true class for X, J(X,y) is the cross-entropy cost functionof the neural network without the network weights and parameters, and Clip<sub>X,ε</sub> {X'} is the function that performs the per-pixel clipping to keep the image in the L<sub>inf</sub> neighborhood of the original image [\\[2\\]](https://arxiv.org/pdf/1607.02533.pdf).\n",
    "    \n",
    "**The Projected Gradient Descent** (PGD) method is also a multi-step variant of the FGSM in which the negative loss function is used to create an adversarial image:\n",
    "\n",
    "<p style=\"text-align: center;\">x<sup>t+1</sup> = Π<sub>x+S</sub>(x<sup>t</sup>+</p>α sign (∇<sub>x</sub>L(0,x,y)))</p>  \n",
    "[\\[3\\]](https://arxiv.org/pdf/1706.06083.pdf)\n",
    "\n",
    "For our experiment, our goal is to determine the effectiveness of these three types of attacks againsts an Athena ensemble composed of the first 20 wd's. To do this, the adversarial examples are constructed by manipulating the tunable parameters within the attacks and then evaluated one by one on the Athena ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Settings  \n",
    "\n",
    "#### Design\n",
    "The files needed to run the craft AE's are conveniently packaged in the Task1 directory. One must simply run \"craft_adversarial_examples.py\" to create a new set of adversarial examples. This will by default use the config, model, and attack jsons found within the Task1/configs and Task1/models directories. \n",
    "- configs/athena-mnist.json\n",
    "    - contains list of weak defenses and which defenses are active\n",
    "- configs/attack-zd-mnist.json\n",
    "    - contains the total attacks to be used in the experiment and \n",
    "- configs/data-minerva-mnist.json\n",
    "    - file contains the specific attack files to be evaluated\n",
    "- models/model-mnist.json\n",
    "    - file containing the default settings of the model\n",
    "    \n",
    "    \n",
    "#### Saving Logs\n",
    "By default the system does not save the results to any output. In order to save results or AE files one must set the corresponding Boolean value to True and specify the output directory in the --output-root argument.\n",
    "    \n",
    "\n",
    "#### CW Attacks\n",
    "The CW Attacks all used the L<sub>∞</sub> configuration. This is our first experience with these types of attacks, therefore, we have decided to hold one variable as a constant while gradually increasing the strength of the other variable.  \n",
    "\n",
    "The specific values used:\n",
    "* epsilon constant: .10, lrs: .2, .4, .6, .8\n",
    "* lr constant: .10, epsilons: .2, .4, .6, .8\n",
    "\n",
    "#### BIM Attacks\n",
    "The BIM attacks are simply varied by their epsilon strength. Initially, it was found to be incrementing by 20%, so that the CW attacks and the BIM attacks easily fooled the undefended model with a 100% success rate. Therefore, lower epsilons were implemented to provide an increasing rate of effectiveness in deception, while anticipating a similar increase in adversarial robustness when the AE's were introduced into the Athena and PGD models.\n",
    "\n",
    "The specific values used:\n",
    "* epsilons: .01, .05, .10, .15, .20\n",
    "\n",
    "#### PGD Attacks\n",
    "Similar to the BIM attacks, the values of the epsilon chosen for the PGD attacks were also chosen at low incremental values. These attacks seemed to fool the undefended model much easier than the other two attacks and as such, the epsilons chosen were extremely small.\n",
    "\n",
    "The specific values used:\n",
    "* epsilons: .025, .05, .075, .10, .15\n",
    "\n",
    "### Creating the AE's\n",
    "In order to create the AE's the json \"attack-zk-mnist.json\" was created with the variables for all 18 attacks. This json was then loaded into craft_adversarial_examples.py to create the adversarial example (.npy) files. To do so, craft_adversarial_examples.py first loads the model, data, and attack json files and the corresponding true labels of the original images. It then creates individual attacks by invoking the generate method of the attack.py factory. \n",
    "\n",
    "Once fully generated, the AE's were saved to output directory, /results, and the files were named per the descriptions relevant to that individual configuration.  \n",
    "\n",
    "model = ../configs/experiment/model-mnist.json  \n",
    "data = ../configs/experiment/data-mnist.json  \n",
    "labels also retrieved from the data-mnist.json  \n",
    "attack_configs = ../configs/experiment/attack-zk-mnist.json  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the AE's against the Models\n",
    "Once the AE's had been created and saved, the AE file names were included in the data-mnist.json in order to evaluate the effectiveness of the attacks against three distinct models.\n",
    "\n",
    "1. Undefended Model  \n",
    "    - The undefended model is a blank model that has no adversarial robustness training. It can identify the numbers 0-9 effectively, given the pictures do not have an adversarial attack transformations.\n",
    "2. The Athena Framework  \n",
    "    - Our Athena framework consisted of the config1 - config20 weak defences integrated into an ensemble. There was no logical reasoning for using these defences, they were just chosen to have a decently large selection from the available weak defences.\n",
    "3. A PGT-ADT Trained Model  \n",
    "    - This was the baseline defence model used to compare against the robustness of the Undefended model and the Athena ensemble.\n",
    "    \n",
    "The robustness results were calculated using the model's guess compared to the true labels of the AE. These results were then output to a file for data collection and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "[comment]: <> (TODO-DS: Must finish descriptions for all the images)\n",
    "\n",
    "*Carlini - Wagner (CW)*\n",
    "\n",
    "As previously stated, our approach for the CW attacks consists of making one of the variables remain constant while we gradually increase the value of the other variables. The graphs below illustrate the resulting differences using this method.\n",
    "\n",
    "\n",
    "The graph below describes the efficiency of the CW attacks on an Ensemble versus a PGD model utilizing a constant epsilon value. Notice the error rate of the PGD model drops slightly while the error rate of the Ensemble remains constant as the Lw increases.\n",
    "\n",
    "![CW-2-Const-Eps](Img/CW-2-Const-Eps.png)\n",
    "<div style=\"text-align: center\">\n",
    "    <em>Figure 1.0  CW Attack effectiveness on an Ensemble vs. PGD with a constant epsilon.</em>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "The line graph below displays the effectiveness of the CW attacks on an Ensemble versus a PGD model using a constant Lw value. The error rate of the PGD model increases linearly as epsilon elevates. On the other hand, the error rate of the Ensemble model only starts to increase after the epsilon value exceeds 0.6. \n",
    "\n",
    "![CW-2-Const-Lw](Img/CW-2-Const-Lw.png)\n",
    "<div style=\"text-align: center\">\n",
    "    <em>Figure 1.1  CW Attack effectiveness on an Ensemble vs. PGD with a constant Lw.</em>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "The graph below shows the effectiveness of the CW attack on an UM, an Ensemble and a PGD mode with a constant epsilon value. Here, the error rate of the UM surpasses the rate of the Ensemble and the PGD models, both of which remain at a constant value as Lw increases. \n",
    "\n",
    "![CW-All-Const-Esp](Img/CW-All-Const-Eps.png)\n",
    "<div style=\"text-align: center\">\n",
    "    <em>Figure 1.2  CW Attack effectiveness on an UM vs. Ensemble vs PGD with a constant epsilon.</em>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Similar to the graph above, the graph below describes the effectiveness of a CW attack on an UM, Ensemble and a PGD model, instead, with a constant Lw value. The error rate of the UM, yet again,  far exceeds the error rate of both the Ensemble and the PGD models.\n",
    "\n",
    "![CW-All-Const-Lw](Img/CW-All-Const-Lw.png)\n",
    "<div style=\"text-align: center\">\n",
    "    <em>Figure 1.3  CW Attack effectiveness on an UM vs. Ensemble vs PGD with a constant Lw.</em>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "*Basic Iterative Method (BIM)*\n",
    "\n",
    "The BIM attacks were administered by modifying the epsilon values. The following graphs depict the results when applying this method to the models.  \n",
    "\n",
    "The line graph below describes the effectiveness of the BIM attack on a Ensemble model versus PGD model. As the epsilon is increased, the error rate of the Ensemble increases faster than the error rate of the PGD model.\n",
    "\n",
    "![BIM-2](Img/BIM-2.png)\n",
    "<div style=\"text-align: center\">\n",
    "    <em>Figure 1.4  BIM Attack effectiveness on an Ensemble vs. PGD.</em>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "The line graph below illistruates the effectiveness of the BIM attack on an UM verses Ensemble model versus PGD model. As the epsilon is increased, the error rate of the UM increases faster than the error rate of the Ensemble and PGD models.\n",
    "\n",
    "![BIM-All](Img/BIM-All.png)\n",
    "<div style=\"text-align: center\">\n",
    "    <em>Figure 1.5  BIM Attack effectiveness on an UM vs. Ensemble vs. PGD.</em>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "*Projected Gradient Descent (PGD)*\n",
    "\n",
    "The PGD attacks were also conducted utilizing modified epsilon values, however, these values are minute. The graphs below feature the results from implementing this technique on our models. \n",
    "\n",
    "The following graph depicts the PGD attack effectiveness on an Ensemble versus a PGD model. The PGD attack is clearly more efficient in fooling the PGD model than the Ensemble. \n",
    "\n",
    "![PGD-2](Img/PGD-2.png)\n",
    "<div style=\"text-align: center\">\n",
    "    <em>Figure 1.6  PGD Attack effectiveness on an Ensemble vs. PGD.</em>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "The following graph displays the effectiveness of a PGD attack on an UM, an Ensemble and a PGD. Ultimately, the PGD attack is the most effective attack against a UM. Its error rate outweighs both the Ensemble and the PGD models.\n",
    "\n",
    "![PGD-All](Img/PGD-All.png)\n",
    "<div style=\"text-align: center\">\n",
    "    <em>Figure 1.7  PGD Attack effectiveness on an UM vs. Ensemble vs. PGD.</em>\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data:\n",
    "\n",
    "Can be seen in Task1/results/data.xlsx and Task1/results/minerva_AE-results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In conclusion to this experiment, we have deduced that PGD or Projected Gradient Descent attacks are the most effective method when used to deceive an undefended model. Due to producing the highest rate of errors as a result of using extremely small epsilon values, PGD’s success rate was strongly considered over BIM or Basic Iterative whose attacks were most efficient when epsilon values elevated and CW or Carkini-Wagner whose attacks were most effective when maintaining a constant epsilon denoting a gradual error rate increase. Despite the overall 100% success rate of all three attacks presented, PGD was still able to exhibit the highest rate of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Contribution\n",
    "\n",
    "This task would not have been possible without contribution from each team member. All members contributed to coding, testing, and reporting. Please note that while not everyone has commits in GitHub we collaborated over Discord and shared code there and just had a single person commit so we all had the same exact code. The most crucial, and time dependent, section of this project was the testing. Each member helped to generate multiple test results (in some cases multiple computers were running tests for each member) to ensure the maximum amount of test data was generated for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "\\[1\\] Carlini, B., Wagner, D. Towards Evaluating the Robustness of Neural Networks. arXiv reprint arXiv:1608.04644 (2017)  \n",
    "\\[2\\] Kurakin, A., Goodfellow, I., Bengio, S. Adversarial Examples in the Physical World arXiv reprint arXiv:1607.02533 (2017)  \n",
    "\\[3\\] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A. Towards Deep Learning Models Resistant to Adversarial Attacks. arXiv reprint arXiv:1706.06083 (2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
